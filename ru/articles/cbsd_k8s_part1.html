<!--# include file="/ru/_start.html" -->
<h1><span>Деплой kubernetes кластера на FreeBSD/bhyve (<strong>CBSD</strong>)</span></h1>
<h2><a name="k8s_part1_intro">Вступление</a></h2>
<div class="block">
<p>Недавно на своей новой работе я с коллегами отпраздновал нестандартную историю успеха - в нашей компании мы полностью избавились от докера (более двухсот контейнеров - немного, не все же).
Да-да, тогда как этот тренд держится уже десяток лет, процесс де-докеризации принес много радости и облегчения. Хоть и временно ;-)</p>
<p>Я не буду говорить сейчас о плюсах и минусах докера - напомню лишь общеизвестную истину - нет ничего идеального и не всегда инструмент или 
реализация того или иного решения является для вас наиболее подходящим. Кроме этого, в сфере IT не существует одного единого верного пути - всегда есть варианты,
чем наша сфера и увлекательна. В какой-то момент времени внутри компании поняли, что используют докер неправильно и решили это исправить, 
переведя все бизнес процессы на рельсы микросервисной архитектуры. Для этого, разработчикам приходится учитывать специфику и особенности контейнерного подхода.
Эти вопросы выходят за рамки статьи, но скажу лишь, что новая концепция и предпринимаемые шаги подразумевают для нас использование контейнеров в еще более крупных 
масштабах (когда наши приложения будут по-настоящему к этому готовы), в связи с чем остро возник вопрос об оркестрации этого хозяйства.</p>
<p>Существует достаточно много решений для оркестрации контейнерами, но наиболее популярный (или наиболее известный и разрекламированный, наверное - это кубернетис).
Поскольку я планирую провести много эксперементов с инсталляциями и конфигурацией k8s, мне потребуется лаборатория, в которой я могу быстро и легким для себя образом разворачивать кластер в любых количествах.
В своей работе и повседневной жизни, я очень плотно использую две ОС - Linux и FreeBSD. Кубер и докер - это Linux-центричные проекты и на первый вгляд, какого-то полезного участия и помощи
от FreeBSD здесь ждать не приходится. Как говорится, из мухи можно сделать слона, но летать он уже не будет. Однако на ум приходят две соблазнительные вещи - это очень хорошая интеграция и работа в FreeBSD файловой системы ZFS,
от которой было бы неплохо использовать механизм снапшотов, COW и надежности. Второе - гипервизор bhyve, поскольку нам все таки нужен загрузчик докеров и кубера в виде Linux ядра. Таким образом, нам необходимо различными 
способами связать воедино определенное количество действий, большинство из которых связано с запуском и предварительной конфигурацией виртуальных машин. Это характерно как для сервера на базе Linux, так и FreeBSD -
что будет работать под капотом для запуска виртуальных машин - большой роли и разницы нет. Давайте возьмем FreeBSD!</p>
<p>В свободное время я являюсь участником <strong>CBSD</strong>, проекта, который одной части конечным пользователям может предоставлять более дружелюбный
интерфейс по управлению контейнерами и виртуальными машинами на платформе FreeBSD. Вторая часть пользователей знает <strong>CBSD</strong> как фреймворк,
который благодаря API очень легко встраивать в любые собственные сценарии по автоматическому управлению виртуальной инфраструктурой, отдав большинство
низкоуровневых операций по работе с ВМ на совесть скриптов CBSD. Таким образом, наша задача сводится к построению некоторо моста между операциями над k8s кластером и
задами, относящимся к деплою виртуальных машин. Работа заняла у меня около 4 часов и превратилась в рабочий k8s модуль <strong>CBSD</strong>,
который доступен в паблике, как и все остальное, связанное с проектом <strong>CBSD</strong>.</p>
<p>Итак, посмотрим что получилось!</p>
<p><em>Спойлер: если вы предпочитаете смотреть видео, небольше демо на Ютубе: <a target="_blank" href="https://youtu.be/ADBuUCtOF1w">https://youtu.be/ADBuUCtOF1w</a></em></p>
</div>
<h2><a name="k8s_part1_at_the_beggining">Что у нас есть</a></h2>
<div class="block">
<p>Я преследую цель поднять небольшую локальную лабораторию с рабочим k8s кластером и в моем распоряжении есть сервер со следующими характеристиками:</p>
<ul>
	<li>1 жесткий диск объемом 3 Tb, на который и установлена FreeBSD 13-CURRENT ( ZFS-on-root )</li>
	<li>RAM: 256 GB</li>
	<li>CPU: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz (56 core)</li>
</ul>
<p>Те, дефицита в ресурсах нет. Используя cloud-образ Ubuntu 18, я сгенерировал cloud-образ kubernetes, который отличается от Ubuntu образа лишь тем, что в нем было выполнено подключение
репозитория kubernetes и выполнена команда <strong>apt install kubeadm</strong>, чтобы при разворачивании образа можно было сразу приступать к работе с утилитой kubectl.</p>
<p>Эксперементальный образ k8s я закачал на зеркала <strong>CBSD</strong> проекта. Запустить виртуальную машину из этого образа вы можете через стандартный диалог CBSD и команду bconstruct-tui:</p>
<p class="text-center"><img src="/img/k8s_boot1.png" alt="" /></p>
<p>Сначала нам необходимо выбрать тип гостя - в случае с k8s наш гость - это Linux:</p>
<p class="text-center"><img src="/img/k8s_boot2.png" alt="" /></p>

<p>Далее, выберем в списке готовых профилей Linux название, содержащее kubernetes - это cloud образ, поэтому ищите его внизу списка:</p>

<p class="text-center"><img src="/img/k8s_boot3.png" alt="" /></p>

<p>Редактируем остальные параметры, если настройки по-умолчанию вас не устраивают:</p>
<p class="text-center"><img src="/img/k8s_boot4.png" alt="" /></p>
<p>Мы можем изменить, например, сетевые настройки хелпера cloud-init:</p>
<p class="text-center"><img src="/img/k8s_boot5.png" alt="" /></p>
<p>После редактирования параметров, отправляем виртуальную машину создаваться:</p>
<p class="text-center"><img src="/img/k8s_boot6.png" alt="" /></p>

<p>На выходе мы получаем рабочую виртуальную машину, в который уже установлен kubernetes, но на этом базовые возможности <strong>CBSD</strong> по части
kubernetes на этом и заканичается - она сфокусирована для других задач.</p>

<p class="text-center"><img src="/img/k8s_boot7.png" alt="" /></p>

<p>Впрочем, это уже хорошо - благодаря cloud-init вам ненужно устанавливать каждый раз ОС используя инсталлятор, 
а также не требуется каждый раз после установки ОС выполнять установку kubernetes - вам только нужно запустить ВМ из gold-образа.
Если вы любите в некоторых случаях выполнять ручную работу - 20 секунд запуска виртуальной машины в любой конфигурации - и можете приступать.
</p>

<p class="text-center"><img src="/img/k8s_boot8.png" alt="" /></p>

<p>Поскольку создавать и настраивать k8s - задача также не из тривиальных (в первую очередь тем, что это большое количество монотонной однообразной обезъяней работы в виде повторения большого количества одинаковых команд), я поместил
эти шаги в отдельные скрипты, которые получили статус модуля k8s для <strong>CBSD</strong>, все операции которого начинаются с одноименного префикса <strong>k8s</strong>.
Все команды <strong>CBSD</strong> имеют вывод информации о своих обязательных или не очень аргументах с кратким описанием через аргумент <strong>--help</strong>.
Модуль предлагает небольшое количество операций, среди которых есть bootstrap, join, конфигурирование и получение токена</p>
Эти операции вы можете запускать в строгой очередности одну за другим, либо можете скормить все необходимые аргументы за 1 раз - модуль сам разберет когда и что сделать.
Например, давая команду:

<pre class="brush:bash;ruler:rule;">
cbsd k8s bootstrap=1 join=1 token=1 setup="master node"
</pre>

мы просим модуль провестю весь цикл по инициализации одного кластера, с экспортом токена для работы с kubectl используя CLI через удаленный машину:</p>

<p class="text-center"><img src="/img/k8s_boot9.png" alt="" /></p>

<p>Модуль <strong>CBSD</strong> выполняет свою работу в различных местах - параллельно, в некоторых - последовательно.</p>
<p>Например, каждый сценарий выполняется в параллельном режиме, но прежде чем модуль сможет перейти к следующему пункту, ему необходимо удостоверится,
что самая последняя или медленная нода закончила настройку и готова к дальнейшему шагу. Таким образом, финальный шаг каждого отдельно взятого сценария -
ожидание. Например, факт распараллеливания и прогресс внутренних задач мы можем наблюдать в выводе <strong>cbsd taskls</strong>:</p>

<p class="text-center"><img src="/img/k8s_boot11.png" alt="" /></p>

<p>Рано или поздно ( в моем случае - через 20 секунд ) модуль перешел от стадии bootstrap к стадии конфигурирования, когда одной ноде поручается роль master,
все остальные - становятся worker.
Цель k8s bootstrap - запустить из gold образа виртуальную машину с kubernetes - то, что выше мы сделали через cbsd bconstruct-tui, запустив одну виртуальную
машину из диалогового интерфейса. Сейчас же, за 20 секунд мы запустили 10 виртуальных машин и их можно увидеть через стандартный вывод <strong>cbsd bls</strong>:</p>

<p class="text-center"><img src="/img/k8s_boot10.png" alt="" /></p>

<p>Надо сказать, что роль <strong>CBSD</strong> здесь - это не только запуск виртуальных машин и работа с cloud-init, но также автоматический поиск свободных
имен для новых контейнеров (в данном случае используется маска kube в качестве начала имени контейнера) и поиск и автоматические назначение свободных IP адресов, которые
система берет из настройки CBSD node ip pool. На скриншоте видно, что IP адреса 10.0.0.XX которые выданы виртуальным машинам, не всегда идут последовательно - некоторые
из них уже заняты в сети.</p>

<p>Стадия конфигурирования master/worker самая долгая и проходит за 3 минуты:</p>
<p class="text-center"><img src="/img/k8s_boot12.png" alt="" /></p>

<p>После чего наступает стадия join, отнимающая 42 секунды нашей жизни. И наконец скрипт, экспортировав токен, завершается успехом:</p>

<p class="text-center"><img src="/img/k8s_boot13.png" alt="" /></p>

<p>Поскольку мы экспортировали (token=1) токен для работы с k8s на локальную машину, мы можем работать с кластером непосредственно с FreeBSD хоста.
Мы можем сразу выполнить команду 'kubectl get nodes' и убедится, что все на месте.</p>

<p class="text-center"><img src="/img/k8s_boot14.png" alt="" /></p>

<p>Итак, кластер готов к работе. Для проверки работоспособности, вы можете применить семпл с официальной документации k8s и запустить в новом кластере ваш первый контейнер:</p>
<p class="text-center"><img src="/img/k8s_boot15.png" alt="" /></p>

<p>Для удаления есть аргумент destroy=1 ( либо удаляйте стандарной командой cbsd bremove ). Также, вы можете сбросить настройки кластера не уничтожая виртуальные машины ( reset=1 ).
<strong>Энжой!</strong>
<p><em>Небольшой подкаст с демо материалами этой статьи на Youtube</em>: <a href target="_blank">https://youtu.be/ADBuUCtOF1w</a></p>
<br>
<h2><a name="k8s_part1_whatnext">Выводы и дальнейшие планы</a></h2>
<p>В условиях энергично развивающейся IT отрасли, крайне важно автоматизировать все возможные операции и получать результат ASAP - цените свое время и нервы и используйте
эти ограниченные и самые ценные ресурсы на то, чтобы выполнять свои собственные задачи.
В этой статье был продемонстрировано сотрудничество ряда технологий: FreeBSD. Linux, ZFS, cloud-init, bhyve, CBSD,
которые призваны сделать одно - поднять готовый к использованию кластер kubernetes любой конфигурации настолько быстро, насколько это возможно.</p>
<p>Очевидно, что наиболее длительный шаг (configure/setup) здесь также можно оптимизировать, поскольку данный шаг выполняет различные действия, работающие с удаленными 
ресурсами (wget .yaml конфигураций для calico и т.д) - это можно также включить в cloud образ kubernetes в дальнейшем.</p>
<p>Также, из планов достаточно интересной кажется идея объединить на счет VXLAN несколько физических серверов с FreeBSD/CBSD, создав единый L2 сегмент для
всех контейнеров. Использование <strong>p9fs</strong>, <strong>Ceph, NFS или S3 объектного хранилища</strong> здесь также имеет право на существование, для организации
единого дискового пространства для контейнеров на разных физических нодах</p>. А также, при должной доработке, на базе FreeBSD и ZFS, мы можем получить отказоустойчивый
масштабируемый кластер кубернетес в виде черного ящика - имеется ввиду не закрытость продукта, а самодостаточный и самостоятельный фреймворк, который не требует
IT специалиста погружаться в технологии FreeBSD, bhyve и CBSD, а получает универсальный API для контроля и управления K8S кластерами.</p>

<p>Кому интересны FreeBSD/bhyve + kubernetes и есть желание запустить собственный SaaS K8S сервис на своих ресурсах с использованием CBSD - пишите идеи и комментарии.
Было бы интересно предоставить публике легкий и дешевый способ для этих операций.</p>
