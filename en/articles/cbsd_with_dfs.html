<!--# include file="/ru/_start.html" -->
<h1><span>Use of <strong>CBSD</strong> together with DFS (Distributed File System)</span></h1>
<div class="block">

<h2><a name="dfs_generic">General information</a></h2>

<p>One of the distinguishing features of <strong>CBSD</strong> from other modern wrappers for managing jail and bhyve on the FreeBSD platform is the lack of a rigid binding to the ZFS file system,
which leads to a certain overhead in terms of code when you use only ZFS, but makes <strong>CBSD</strong> a more versatile tool that you can use in more general situations. </p>
<p>One such situation is the use of various embedded platforms with very few resources where the ZFS file system is redundant and voracious,
which makes it ineffective on various Raspberry PI and similar solutions. On the opposite side of the minimalism are large and large-scale hyperconvergent installations using NAS/SAN
and distributed storage systems, using external storage that is connected via NFS/iSCSI protocol or distributed storage systems like ClusterFS and Ceph.</p>

<p>This will highlight the use of <strong>CBSD</strong> in these installations and describe the Howto-style application notes.</p>

<p>The general requirement for using <strong>CBSD</strong> on DFS, which is typical for any implementations, is turning off the <em>zfsfeat</em> option and <em>hammerfeat</em> option in 'cbsd initenv-tui' 
and the need to bring the following directories to the shared store:</p>

<ul>
	<li>~cbsd/jails-data: directory with container or virtual machine data</li>
	<li>~cbsd/jails-system: system directory with additional system information related to the container or virtual machine</li>
	<li>~cbsd/jails-rcconf: the directory is used when the environment switches to unregister mode</li>
	<li>~cbsd/jails-fstab: the directory for fstab files</li>
</ul>

<p>If the working directory  (<strong>workdir</strong>) is initialized in <em>/usr/jails</em> this is, respectively, the directories:</p>

<pre class="cli">
/usr/jails/jails-data
/usr/jails/jails-system
/usr/jails/jails-rcconf
/usr/jails/jails-fstab
</pre>

<p>You can transfer the indicated locations to DFS and mount all the environments at one place, however, the authors recommend dividing the resources into individual environments.
For example, for container jail1, export and mount directly jail1 data, separate from others:

<pre class="cli">
/usr/jails/jails-data/jail1-data
/usr/jails/jails-system/jail1
/usr/jails/jails-fstab/jail1
</pre>

<p>This will allow you to optimize the operation of file systems between environments and reduce the potential impact of one environment on another -
 for example, if jail1 very aggressively uses file system locks, it will also affect jail2 when mounting /usr/jails/jails-data, whereas different sessions and mounts 
 for /usr/jails/jails-data/jail1-data and /usr/jails/jails-data/jail2-data will be independent. 
 In addition, in this case, you can use different mount sources - jail1 resources can be located on one DFS server, and jail2 on another. 
 For this, <strong>CBSD</strong> has a mechanism for mounting and dismounting, as described below.</p>

Other directories, such as bases, you can also put on a shared volume to save space.
However, it is much more efficient to store the base container files locally, which with the <strong>baserw=0</strong> parameter guarantees the operation of the basic utilities and libraries
with the speed of the local disk and the absence of possible network problems.</p>

<p>Shared storage provides an easy way to migrate a zero-copy environment. So, you can move the container to the <strong>unregister</strong> state on one node:</p>

<p><strong>node1</strong>:</p>
<pre class="brush:bash;ruler:rule;">
% cbsd junregister jname='*'
</pre>

<p>and having registered, without any copying, start using it on another:</p>

<p><strong>node2</strong>:</p>
<pre class="brush:bash;ruler:rule;">
% cbsd jregister jname='*'
</pre>

<p>Known issues.</p>

<p>Some DFS, such as NFSv3 and GlusterFS, require additional configuration in pkg.conf for correct locking:</p>

<pre class="brush:bash;ruler:rule;">
% echo "NFS_WITH_PROPER_LOCKING = true;" >> /usr/local/etc/pkg.conf
</pre>

<h2><a name="dfs_cbsd_hooks"><strong>CBSD</strong> hooks for DFS</a></h2>

<p>If you go beyond the case when all the environment data is located in the local ZFS/UFS file system, you will come across the issue of pre-mounting a resource containing the environment data.
To do this, the <strong>CBSD</strong> uses the <strong>mnt_start</strong> and <strong>mnt_stop</strong> parameters, in which you specify user scripts that work at startup and when the environment stops.
It can be any executable script, for successful work with <strong>CBSD</strong> of which it is necessary to follow several rules:
	<ul>
		<li>Your script must be resilient to secondary mounting. For example, you started the environment, then stopped and started again.
		Your script should not be mounted on top of the previous mount - this check and this behavior is on the conscience of your script - check the fact of mounting;</li>
		<li>Your script should exit with code 0 on success or non-zero code on fatal problems;</li>
		<li>Your hook's run time is unlimited - avoid situations where your script may hang forever;</li>
		<li><strong>CBSD</strong> runs a script with certain arguments to convey some useful information. Your script must process the parameters via getopt in order to accept them. These are the following keys:
		<table class="images">
			<tr><td><strong>-d</strong></td><td>data - path to the environment data directory;</td></tr>
			<tr><td><strong>-f</strong></td><td>fstab - path to the fstab directory (relevant mainly for jail);</td></tr>
			<tr><td><strong>-j</strong></td><td>jname - directly the name of your environment;</td></tr>
			<tr><td><strong>-r</strong></td><td>rcconf - rcconf directory path;</td></tr>
			<tr><td><strong>-s</strong></td><td>sysdata - sysdata directory path;</td></tr>
		</table>
		</li>
	</ul>

	<p>As you can see, using these parameters with the ability to create custom scripts for connecting external storage gives you the opportunity to use scripts that no white people have heard of, for example: iSCSI, Infiniband, GELI, HASTD, etc.</p>

	<p>As a working example, look at the script for working with NFSv4 in the next paragraph.</p>

	<p>In addition, it may be useful for you to find out about the possibility of custom cloning operations - for example, 
	if you manage your external storage and it can perform data copying operations on storage side, you can avoid high network load during normal data copying, 
	and make an instant clone with using an external drive. See <a href="/<!--#echo var="lang" -->/<!--#echo var="version" -->/wf_jclone_ssi.html#clone-local.d%20hook:%20local/custom%20data%20clone">here</a> and <a target="_blank" href="https://github.com/cbsd/cbsd/issues/373">here</a> for details.</p>

<h2><a name="dfs_nfs">CBSD with NFSv4</a></h2>

<p>Using <strong>CBSD</strong> and jail with NFSv4</p>
<p>We have a separate NFS server based on FreeBSD and we have a separate server with <strong>CBSD</strong> that will store jail on an NFS server.</p>
<p>IPv4 of NFS server: 172.16.0.1, IPv4 of <strong>CBSD</strong> server: 172.16.0.100</p>
<p>The jail data on the NFS server will be located in the <strong>/nfs</strong> directory</p>
<p>Let's configure NFSv4 server:</p>
<p>In <em>/etc/exports</em> write path for exports:</p>
<pre class="cli">
/nfs -alldirs -maproot=root 172.16.0.100
V4: / 172.16.0.100
</pre>
<p>If the NFS server uses ZFS, be sure to enable the sharenfs option on the required pool. For example, if the ZFS root system is zroot/ROOT/default, then NFS export is enabled through the following command:</p>
<pre class="brush:bash;ruler:rule;">
% zfs set sharenfs=on zroot/ROOT/default
</pre>

<p>In <em>/etc/rc.conf</em> add the services necessary to start the NFS server by running the following commands:</p>

<pre class="cli">
sysrc -q nfsv4_server_enable="YES"
sysrc -q nfscbd_enable="YES"
sysrc -q nfsuserd_enable="YES"
sysrc -q mountd_enable="YES"
sysrc -q nfsuserd_enable="YES"
sysrc -q rpc_lockd_enable="YES"
</pre>

<p>Create empty directories for jail1, which will be on NFS:</p>
<pre class="cli">
mkdir -p /nfs/data/jail1 /nfs/fstab/jail1 /nfs/rcconf/jail1 /nfs/system/jail1
</pre>
<p>After rebooting the NFS server or services, the NFSv4 server is ready for operation.</p>

<p>From the <strong>CBSD</strong> side, we need to mount the container data through the following commands:</p>

<pre class="cli">
% mount_nfs -o vers=4 172.16.0.1:/nfs/data/jail1 /usr/jails/jails-data/jail1-data
% mount_nfs -o vers=4 172.16.0.1:/nfs/fstab/jail1 /usr/jails/jails-fstab/jail1
% mount_nfs -o vers=4 172.16.0.1:/nfs/rcconf/jail1 /usr/jails/jails-rcconf/jail1
% mount_nfs -o vers=4 172.16.0.1:/nfs/system/jail1 /usr/jails/jails-system/jail1
</pre>

<p>You can use the <em>/etc/fstab</em> file with the 'late' option to mount directories when the node starts, or use automount ( <a href="http://man.freebsd.org/autofs/5" target="_blank">autofs(5)</a> )</p>
<p>However, in this example, we will demonstrate the operation of the <strong>mnt_start</strong> option with an external custom script for mounting file systems. 
The script is present as a working example and lies in the directory <em>/usr/local/cbsd/share/examples/env_start/nfsv4</em>. 
It is pretty <a target="_blank" href="https://github.com/cbsd/cbsd/blob/develop/share/examples/env_start/nfsv4/env_start_nfs.sh">simple</a>.
It also uses a configuration file in which you can edit the dynamic settings for your environment.</p>
<p>Copy the configuration file and fix the IP addresses:</p>
<pre class="cli">
% cp -a /usr/local/cbsd/share/examples/env_start/nfsv4/env_start_nfs.conf ~cbsd/etc/
% vi ~cbsd/etc/env_start_nfs.conf
</pre>
<p>In our example, the file will look as follows:</p>
<pre class="cli">
NFS_SERVER="172.16.0.1"
NFS_SERVER_ROOT_DIR="/nfs"
MOUNT_RCCONF=0
MOUNT_FSTAB=1
MOUNT_NFS_OPT="-orw -overs=4 -ointr -orsize=32768 -owsize=32768 -oacregmax=3 -oacdirmin=3 -oacdirmax=3 -ohard -oproto=tcp -otimeout=300"
</pre>

<p>Create jail via cbsd jconstruct-tui:</p>

<p>Do not forget that we create a container with the name jail1, specify it as jname.</p>
<p class="text-center"><img src="/img/cbsd_dfs_nfs1.png" alt="" /></p>
<p>Since we will use the script for the <strong>mnt_start</strong> parameter without modification, 
we will specify it directly in /usr/local/cbsd/share/examples/env_start/nfsv4/env_start_nfs.sh:</p>
<p class="text-center"><img src="/img/cbsd_dfs_nfs2.png" alt="" /></p>
<p>Now, for operations that require container data (for example jstart, jcreate), <strong>CBSD</strong> will call this script.</p>

<p>If you use several <strong>CBSD</strong> servers, you can easily re-register the container from one physical server to another 
through the junregister command on one server and jregister on another:</p>

<pre>
node1 % cbsd junregister jname='jail*'     // on the source node where the jail is registered
node2 % cbsd jregister jname='jail*'       // on another/destrination node
</pre>

<p>And migrate your environments by distributing and scaling the load on your cluster more smoothly.</p>

<h2><a name="dfs_glusterfs">CBSD and GlusterFS</a></h2>

<br>
<p>WIP. Short howto available here: <a href="/<!--#echo var="lang" -->/articles/cbsd_jail_glusterfs.html">CBSD with GlusterFS</a></p>
<br>

<h2><a name="dfs_ceph">CBSD and CEPH</a></h2>

<br>
<p>WIP/<p>
<br>

</div>
<!--# include file="/ru/_end.html" -->
